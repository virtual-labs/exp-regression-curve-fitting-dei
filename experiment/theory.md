### Theory

Regression and curve fitting are statistical techniques used to model the relationship between two or more variables. When we collect data from experiments or observations, the data points may not perfectly align along a straight or smooth curve due to variations, measurement errors, or natural randomness. Curve fitting helps generate a mathematical function that best represents the underlying trend of the data.

In this experiment, two important regression methods are used:

1. **Least-Square Method**
   The Least-Square technique determines the best-fit curve by minimizing the sum of the squared differences (errors) between the observed data and the predicted values. It ensures that the chosen curve has the smallest possible total deviation from the actual data points.

2. **Weighted Least-Square Method**
   This method extends the Least-Square approach by assigning different weights to different data points. Points with higher reliability or importance are given greater weight. This helps improve the accuracy of the fitted model, especially when some data measurements are less precise.

Different regression models may be applied depending on the pattern of data:
- **Linear Regression** fits a straight line, useful for linear relationships.
- **Polynomial Regression** fits a curved line, useful for non-linear data trends.
- **Exponential Regression** models rapid growth or decay behaviors.

The effectiveness of a regression model is often measured using statistical indicators like the **coefficient of determination (RÂ²)**, which shows how well the model explains data variability.

Regression and curve fitting play a vital role in fields such as engineering, machine learning, economics, medical research, and predictive analytics, enabling better decision-making and future forecasting.
